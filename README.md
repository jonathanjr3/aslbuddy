# iOS app to interpret American Sign Language
- iOS app to interpret american sign language using on-device ML model through device's camera, submitted for hacksheffield[9] hackathon.
- Winner of Frasers Group and SWICS themes

### Tech used:
- CoreML (To run the model on-device)
- CreateML (To create and train the models on macOS)
- Swift
- SwiftUI

Check it out on [devpost](https://devpost.com/software/bsl-buddy)
